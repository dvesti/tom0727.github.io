+++
title = '初学爬虫小记'
date = 2021-02-01T22:45:03+08:00
draft = false
categories = ['技术']
tags = ['爬虫', 'python']
+++

<del>写博客好累啊，是我太久没有写作了吗</del>

这次来记录一下我第一次学习爬虫的经历

## 起因
补完とにかくかわいい的番，感觉真好看啊，漫画也不错，就打算补补[とにかくかわいい的生肉漫画](https://manga1001.com/%E3%80%90%E7%AC%AC1%E8%A9%B1%E3%80%91%E3%83%88%E3%83%8B%E3%82%AB%E3%82%AF%E3%82%AB%E3%83%AF%E3%82%A4%E3%82%A4-raw/)，但是这网站广告特别多，还会检测我adblocker，禁用javascript的话漫画就加载不出来了，气死我了，一怒之下决定学习爬虫把漫画爬下来看。

## 过程
爬虫教程有很多，这里特别推荐一个Jack Cui的教程：
> [[资源分享]     Python3 网络爬虫：漫画下载，动态加载、反爬虫这都不叫事](http://www.soolco.com/post/73836_1_1.html)

## 爬 manga1001.com
这个网站设置的比较粗糙，图片都是静态加载的(F12就能看见图片链接)，根据标签`soup.find_all()`一下即可。对于这个网站的话，简单说一下爬虫的基本流程吧。

{{% question 基本流程 %}}
1. 观察一下页面的HTML，用F12打开可以看到大致结构，如果要看源代码的话，可以选择：
   
   1.1. `res = requests.get(url), print(res.txt)` 
   或

   1.2. 在url前加上`view-source:`，然后用浏览器打开。
2. 找到包含图片的tag， 找一下规律，然后用 `soup.find_all()` 即可。
3. 获取所有章节的URL，然后分别去每个URL里抓取。

{{% /question %}}


{{% fold 参考代码 %}}
```
import requests
from bs4 import BeautifulSoup
import os
import random
import time

def create_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)

root_folder = '/Users/huzhenwei/Desktop/manga/'
create_dir(root_folder)
USER_AGENTS = [
    "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)",
    "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)",
    "Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)",
    "Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)",
    "Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)",
    "Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)",
    "Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0",
    "Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5",
    "Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20",
    "Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52",
]


# get content of one chapter
def get_content(folder, prefix, url):

    res = requests.get(url)
    soup = BeautifulSoup(res.content, 'html.parser')
    items = soup.find_all('figure')

    i = 1

    folder_name = os.path.join(folder, f'Chapter_{prefix:03}/')
    create_dir(folder_name)

    for item in items:

        for child in item.children:
            if i != 1:
                img_url = child.get('data-src')
            else:
                img_url = child.get('src')

            print(img_url)
            headers = random.choice(USER_AGENTS)
            img_html = requests.get(img_url, headers)

            img_name = os.path.join(folder_name, f'{i:02}.jpg')
            with open(img_name, 'wb') as file:
                file.write(img_html.content)
                file.flush()

            i += 1
            time.sleep(random.uniform(0, 3.33))  # sleep random time


# get manga url list
def get_url_list(manga_name, url):
    res = requests.get(url)
    soup = BeautifulSoup(res.content, 'html.parser')

    items = soup.find_all('option')

    for i in range(len(items) - 2, -1, -1):
        if items[i] == items[-1]:  # get manga chapter url list without duplicates
            items = items[i+1:]
            break

    chapter = 1
    folder = os.path.join(root_folder, f'{manga_name}/')

    for item in items:
        manga_url = item.get('value')
        get_content(folder, chapter, manga_url)

        print(manga_url)
        chapter += 1


def main():
    url1 = "https://manga1001.com/%e3%80%90%e7%ac%ac1%e8%a9%b1%e3%80%91%e3%83%88%e3%83%8b%e3%82%ab%e3%82%af%e3%82%ab%e3%83%af%e3%82%a4%e3%82%a4-raw/"
    name1 = 'Tonikaku_Kawaii'
    get_url_list(name1, url1)

    url2 = "https://manga1001.com/%e3%80%90%e7%ac%ac1%e8%a9%b1%e3%80%91%e5%b9%b2%e7%89%a9%e5%a6%b9%e3%81%86%e3%81%be%e3%82%8b%e3%81%a1%e3%82%83%e3%82%93-raw/"
    name2 = 'Umaru_Chan'
    get_url_list(name2, url2)

main()
```
{{% /fold %}}

> 注: 这里用的`USER_AGENT`和`sleep()`都是为了防止被发现然后封IP

## 爬 manhuagui.com

上面那个太没挑战性了，于是我打算再爬一个。

打开[漫画网站](https://www.manhuagui.com/comic/27099/354852.html)，
![image](/images/002/1.png)
发现没有图片链接，说明是动态加载的图片(用javascript加载的)，那怎么办呢？

### Step 1
首先我们要获得某一话的所有图片链接，可以从图上看出似乎有一大段像是加密后的字符串，我们打开[第一话](https://www.manhuagui.com/comic/27099/354390.html)和[第二话](https://www.manhuagui.com/comic/27099/)的HTML，用命令行`diff`一下以后，会发现差异就刚好出现在这串字符串上：![image](/images/002/2.JPG)我们可以肯定这里面包含了图片链接相关的信息。

### Step 2
<del>太晚了，睡觉去了，明早起来接着写</del>


