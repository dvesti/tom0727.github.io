[{"categories":["算法"],"contents":"介绍 给定方程组： $$\\begin{cases} x\\equiv a_1 (\\text{mod }m_1) \\\\\nx\\equiv a_2 (\\text{mod }m_2) \\\\\n\u0026hellip; \\\\\nx\\equiv a_k (\\text{mod }m_k) \\\\\n\\end{cases}$$\n其中 $a_i \\geq 0, m_i \u0026gt; 0, a_i,m_i \\in \\mathbb{Z}$, 且 $m_i$ 之间两两互质。\n求满足条件的最小正整数解 $x$ ？\n结论 令 $M = m_1 \\cdot m_2 ~\u0026hellip; ~m_k$, $M_i = \\frac{M}{m_i}$\n令 $M_i^{-1}$ 为 $M_i$ 在 $\\text{mod }m_i$ 意义下的逆元\n则，答案为: $x = \\sum_{i=1}^{k}a_iM_iM_i^{-1} ~ (\\text{mod }M)$\n证明 因为 $\\forall i \\neq j, M_i \\equiv 0 ~(\\text{mod } m_j)$\n所以 $\\forall i \\neq j, a_iM_iM_i^{-1} \\equiv 0~ (\\text{mod } m_j)$\n所以 $\\forall i, x \\equiv \\sum_{i=1}^{k}a_iM_iM_i^{-1} \\equiv a_iM_iM_i^{-1}$\n又因为 $\\forall i, M_iM_i^{-1} \\equiv 1 ~(\\text{mod } m_i)$\n所以 $\\forall i, x \\equiv a_iM_iM_i^{-1} \\equiv a_i ~ (\\text{mod } M_i)$\n证明推导过程 证明推导过程 这里直接放参考链接一中的片段：\n 由于 $M = 3\\times5\\times7 = 105$\n所以最后的解就是 $x = (n_1 + n_2 + n_3) ~\\bmod ~ 105 = 233 ~ \\bmod 105 = 23$\n  例题  https://www.luogu.com.cn/problem/P3868  快速乘 在题目中，有可能出现 $\\bmod$ 接近 $1e18$ 的情况，有可能会爆 long long，所以在计算大数乘法时，要用快速乘（原理类似于快速幂）：\nll qmul(ll a, ll b) { ll ans = 0; while (b) { if (b \u0026amp; 1) ans = (ans + a) % mod; b \u0026gt;\u0026gt;= 1; a = (a + a) % mod; } return ans; }  模版 luogu-P3868-AC代码 #include \u0026lt;bits/stdc++.h\u0026gt; #define fastio ios::sync_with_stdio(false); cin.tie(0); #define ll long long ll M = 1; ll qmul(ll a, ll b) { ll ans = 0; while (b) { if (b \u0026amp; 1) ans = (ans + a) % M; b \u0026gt;\u0026gt;= 1; a = (a + a) % M; } return ans; } ll exgcd(ll a, ll b, ll\u0026amp; x, ll\u0026amp; y) { if (!b) { x = 1, y = 0; return a; } ll g = exgcd(b, a%b, x, y); ll curx = y; ll cury = x - (a/b) * y; x = curx; y = cury; return g; } ll a[15], b[15]; int k; ll crt() { ll ans = 0; for (int i = 1; i \u0026lt;= k; i++) { ll m = M / b[i]; ll x,y; exgcd(m, b[i], x, y); if (x \u0026lt; 0) x += b[i]; ans = (ans + qmul(qmul(a[i], m), x)) % M; } return ans; } int main() { fastio; cin \u0026gt;\u0026gt; k; for (int i = 1; i \u0026lt;= k; i++) cin \u0026gt;\u0026gt; a[i]; for (int i = 1; i \u0026lt;= k; i++) cin \u0026gt;\u0026gt; b[i], M *= b[i]; for (int i = 1; i \u0026lt;= k; i++) { a[i] -= (a[i]/b[i]) * b[i]; //将a[i]变成正数 a[i] += b[i]; a[i] %= b[i]; } cout \u0026lt;\u0026lt; crt() \u0026lt;\u0026lt; endl; }   拓展中国剩余定理 (excrt) 用于 $m_1,m_2,\u0026hellip;,m_k$ 并不互质的情况，略\n参考链接  https://zhuanlan.zhihu.com/p/103394468  ","date":"2021-02-06T10:46:42+08:00","permalink":"https://tom0727.github.io/post/003-crt/","tags":["数学","中国剩余定理"],"title":"中国剩余定理介绍"},{"categories":["工程"],"contents":"写博客好累啊，是我太久没有写作了吗\n这次来记录一下我第一次学习爬虫的经历\n起因 补完とにかくかわいい的番，感觉真好看啊，漫画也不错，就打算补补とにかくかわいい的生肉漫画，但是这网站广告特别多，还会检测我adblocker，禁用javascript的话漫画就加载不出来了，气死我了，一怒之下决定学习爬虫把漫画爬下来看。\n爬虫教程有很多，这里特别推荐一个Jack Cui的教程：\n [资源分享] Python3 网络爬虫：漫画下载，动态加载、反爬虫这都不叫事\n 爬 manga1001.com 这个网站设置的比较粗糙，图片都是静态加载的(F12就能看见图片链接)，根据标签soup.find_all()一下即可。对于这个网站的话，简单说一下爬虫的基本流程吧。\n基本流程\n  观察一下页面的HTML，用F12打开可以看到大致结构，如果要看源代码的话，可以选择：\n1.1. res = requests.get(url), print(res.txt) 或\n1.2. 在url前加上view-source:，然后用浏览器打开。\n  找到包含图片的tag， 找一下规律，然后用 soup.find_all() 即可。\n  获取所有章节的URL，然后分别去每个URL里抓取。\n   参考代码 import requests from bs4 import BeautifulSoup import os import random import time def create_dir(path): if not os.path.exists(path): os.makedirs(path) root_folder = '/Users/huzhenwei/Desktop/manga/' create_dir(root_folder) USER_AGENTS = [ \u0026quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\u0026quot;, \u0026quot;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\u0026quot;, \u0026quot;Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\u0026quot;, \u0026quot;Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\u0026quot;, \u0026quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\u0026quot;, \u0026quot;Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\u0026quot;, \u0026quot;Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\u0026quot;, \u0026quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\u0026quot;, \u0026quot;Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\u0026quot;, \u0026quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\u0026quot;, \u0026quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\u0026quot;, \u0026quot;Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\u0026quot;, \u0026quot;Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\u0026quot;, \u0026quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\u0026quot;, \u0026quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\u0026quot;, \u0026quot;Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52\u0026quot;, ] # get content of one chapter def get_content(folder, prefix, url): res = requests.get(url) soup = BeautifulSoup(res.content, 'html.parser') items = soup.find_all('figure') i = 1 folder_name = os.path.join(folder, f'Chapter_{prefix:03}/') create_dir(folder_name) for item in items: for child in item.children: if i != 1: img_url = child.get('data-src') else: img_url = child.get('src') print(img_url) headers = random.choice(USER_AGENTS) img_html = requests.get(img_url, headers) img_name = os.path.join(folder_name, f'{i:02}.jpg') with open(img_name, 'wb') as file: file.write(img_html.content) file.flush() i += 1 time.sleep(random.uniform(0, 3.33)) # sleep random time # get manga url list def get_url_list(manga_name, url): res = requests.get(url) soup = BeautifulSoup(res.content, 'html.parser') items = soup.find_all('option') for i in range(len(items) - 2, -1, -1): if items[i] == items[-1]: # get manga chapter url list without duplicates items = items[i+1:] break chapter = 1 folder = os.path.join(root_folder, f'{manga_name}/') for item in items: manga_url = item.get('value') get_content(folder, chapter, manga_url) print(manga_url) chapter += 1 def main(): url1 = \u0026quot;https://manga1001.com/%e3%80%90%e7%ac%ac1%e8%a9%b1%e3%80%91%e3%83%88%e3%83%8b%e3%82%ab%e3%82%af%e3%82%ab%e3%83%af%e3%82%a4%e3%82%a4-raw/\u0026quot; name1 = 'Tonikaku_Kawaii' get_url_list(name1, url1) url2 = \u0026quot;https://manga1001.com/%e3%80%90%e7%ac%ac1%e8%a9%b1%e3%80%91%e5%b9%b2%e7%89%a9%e5%a6%b9%e3%81%86%e3%81%be%e3%82%8b%e3%81%a1%e3%82%83%e3%82%93-raw/\u0026quot; name2 = 'Umaru_Chan' get_url_list(name2, url2) main()    注: 这里用的USER_AGENT和sleep()都是为了防止被发现然后封IP\n 爬 manhuagui.com 上面那个太没挑战性了，于是我打算再爬一个。\n打开漫画网站， 发现没有图片链接，说明是动态加载的图片(用javascript加载的)，那怎么办呢？\nStep 1 先在网页里找找链接长啥样，毕竟用浏览器浏览的话，图片总是会被加载出来的，然后就能看到链接了，果然，在chrome的Elements这个tag里，我们翻到了图片链接：\n不过直接把链接复制到浏览器里打开的话会403，所以我们先搁置一下。\nStep 2 我们要获得某一话的所有图片链接，可以从图上看出似乎有一大段像是加密后的字符串，我们打开第一话和第二话的HTML，用命令行diff一下以后，会发现差异就刚好出现在这串字符串上：我们可以肯定这里面包含了图片链接相关的信息。\nStep 3 既然找到了加密串，那就要找一个钥匙来解码，看一下网页里内容不多，看起来并没有其他有用信息了，但是还有几个.js文件，一个个打开来看一下，终于在其中一个文件里找到了一大堆代码，然后这一大堆里面，有一段看起来又被加密了（有点此地无银三百两啊）： 把这段代码复制到chrome的console里，发现被自动解码了，得到了一个js函数： 我们点开这个函数，看一下里面的内容：\n函数内容 var LZString=(function(){var f=String.fromCharCode;var keyStrBase64=\u0026quot;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=\u0026quot;;var baseReverseDic={};function getBaseValue(alphabet,character){if(!baseReverseDic[alphabet]){baseReverseDic[alphabet]={};for(var i=0;i\u0026lt;alphabet.length;i++){baseReverseDic[alphabet][alphabet.charAt(i)]=i}}return baseReverseDic[alphabet][character]}var LZString={decompressFromBase64:function(input){if(input==null)return\u0026quot;\u0026quot;;if(input==\u0026quot;\u0026quot;)return null;return LZString._0(input.length,32,function(index){return getBaseValue(keyStrBase64,input.charAt(index))})},_0:function(length,resetValue,getNextValue){var dictionary=[],next,enlargeIn=4,dictSize=4,numBits=3,entry=\u0026quot;\u0026quot;,result=[],i,w,bits,resb,maxpower,power,c,data={val:getNextValue(0),position:resetValue,index:1};for(i=0;i\u0026lt;3;i+=1){dictionary[i]=i}bits=0;maxpower=Math.pow(2,2);power=1;while(power!=maxpower){resb=data.val\u0026amp;data.position;data.position\u0026gt;\u0026gt;=1;if(data.position==0){data.position=resetValue;data.val=getNextValue(data.index++)}bits|=(resb\u0026gt;0?1:0)*power;power\u0026lt;\u0026lt;=1}switch(next=bits){case 0:bits=0;maxpower=Math.pow(2,8);power=1;while(power!=maxpower){resb=data.val\u0026amp;data.position;data.position\u0026gt;\u0026gt;=1;if(data.position==0){data.position=resetValue;data.val=getNextValue(data.index++)}bits|=(resb\u0026gt;0?1:0)*power;power\u0026lt;\u0026lt;=1}c=f(bits);break;case 1:bits=0;maxpower=Math.pow(2,16);power=1;while(power!=maxpower){resb=data.val\u0026amp;data.position;data.position\u0026gt;\u0026gt;=1;if(data.position==0){data.position=resetValue;data.val=getNextValue(data.index++)}bits|=(resb\u0026gt;0?1:0)*power;power\u0026lt;\u0026lt;=1}c=f(bits);break;case 2:return\u0026quot;\u0026quot;}dictionary[3]=c;w=c;result.push(c);while(true){if(data.index\u0026gt;length){return\u0026quot;\u0026quot;}bits=0;maxpower=Math.pow(2,numBits);power=1;while(power!=maxpower){resb=data.val\u0026amp;data.position;data.position\u0026gt;\u0026gt;=1;if(data.position==0){data.position=resetValue;data.val=getNextValue(data.index++)}bits|=(resb\u0026gt;0?1:0)*power;power\u0026lt;\u0026lt;=1}switch(c=bits){case 0:bits=0;maxpower=Math.pow(2,8);power=1;while(power!=maxpower){resb=data.val\u0026amp;data.position;data.position\u0026gt;\u0026gt;=1;if(data.position==0){data.position=resetValue;data.val=getNextValue(data.index++)}bits|=(resb\u0026gt;0?1:0)*power;power\u0026lt;\u0026lt;=1}dictionary[dictSize++]=f(bits);c=dictSize-1;enlargeIn--;break;case 1:bits=0;maxpower=Math.pow(2,16);power=1;while(power!=maxpower){resb=data.val\u0026amp;data.position;data.position\u0026gt;\u0026gt;=1;if(data.position==0){data.position=resetValue;data.val=getNextValue(data.index++)}bits|=(resb\u0026gt;0?1:0)*power;power\u0026lt;\u0026lt;=1}dictionary[dictSize++]=f(bits);c=dictSize-1;enlargeIn--;break;case 2:return result.join('')}if(enlargeIn==0){enlargeIn=Math.pow(2,numBits);numBits++}if(dictionary[c]){entry=dictionary[c]}else{if(c===dictSize){entry=w+w.charAt(0)}else{return null}}result.push(entry);dictionary[dictSize++]=w+entry.charAt(0);enlargeIn--;w=entry;if(enlargeIn==0){enlargeIn=Math.pow(2,numBits);numBits++}}}};return LZString})();String.prototype.splic=function(f){return LZString.decompressFromBase64(this).split(f)};   获得这个函数以后，我们尝试着把之前获得的加密串放进去看看： 这看起来就正常多了，而且这里面的 04|05|06|... 之类的信息看起来也能和之前找到的图片链接对应上。但是它似乎并没有按照某个特定的规律来，所以可以肯定还有一个函数来处理这个字符串。\nStep 4 有了这个信息，我们就接着找处理这个字符串的函数，再次观察一下HTML，发现这个Base64的串被包含在了一个\u0026lt;script\u0026gt;当中，长这样：\nscript内容 \u0026lt;script type=\u0026quot;text/javascript\u0026quot;\u0026gt;window[\u0026quot;\\x65\\x76\\x61\\x6c\u0026quot;](function(p,a,c,k,e,d){e=function(c){return(c\u0026lt;a?\u0026quot;\u0026quot;:e(parseInt(c/a)))+((c=c%a)\u0026gt;35?String.fromCharCode(c+29):c.toString(36))};if(!''.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return'\\\\w+'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp('\\\\b'+e(c)+'\\\\b','g'),k[c]);return p;}('X.B({\u0026quot;y\u0026quot;:8,\u0026quot;x\u0026quot;:\u0026quot;w v u t s r\u0026quot;,\u0026quot;q\u0026quot;:\u0026quot;8.1\u0026quot;,\u0026quot;p\u0026quot;:o,\u0026quot;n\u0026quot;:\u0026quot;4\u0026quot;,\u0026quot;l\u0026quot;:[\u0026quot;j.1.2\u0026quot;,\u0026quot;A.1.2\u0026quot;,\u0026quot;h.1.2\u0026quot;,\u0026quot;9-a.1.2\u0026quot;,\u0026quot;b.1.2\u0026quot;,\u0026quot;c.1.2\u0026quot;,\u0026quot;d.1.2\u0026quot;,\u0026quot;f.1.2\u0026quot;,\u0026quot;10.1.2\u0026quot;,\u0026quot;11.1.2\u0026quot;,\u0026quot;12.1.2\u0026quot;,\u0026quot;13.1.2\u0026quot;,\u0026quot;14.1.2\u0026quot;,\u0026quot;15.1.2\u0026quot;,\u0026quot;16.1.2\u0026quot;,\u0026quot;17.1.2\u0026quot;,\u0026quot;g.1.2\u0026quot;,\u0026quot;k.1.2\u0026quot;,\u0026quot;C.1.2\u0026quot;,\u0026quot;P.1.2\u0026quot;,\u0026quot;D.1.2\u0026quot;,\u0026quot;U.1.2\u0026quot;,\u0026quot;W.1.2\u0026quot;,\u0026quot;3.1.2\u0026quot;,\u0026quot;%Y%5%7%6%5%Z%6%7%T.1.2\u0026quot;],\u0026quot;V\u0026quot;:R,\u0026quot;Q\u0026quot;:3,\u0026quot;S\u0026quot;:\u0026quot;/O/z/N/4/\u0026quot;,\u0026quot;M\u0026quot;:0,\u0026quot;L\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;K\u0026quot;:J,\u0026quot;I\u0026quot;:H,\u0026quot;G\u0026quot;:{\u0026quot;e\u0026quot;:F,\u0026quot;m\u0026quot;:\u0026quot;E\u0026quot;}}).i();',62,70,'D4KwDg5sDuCmBGZgCYCsxA03gBgIyD21YADgCFgBRdATlOQHYtLLgsAWZ9LANmducOGZMc/LAGZgYAE6wAkgDsAlgBdmOYDiYAzBQBtYAZwEBjOQEMAtrGCjULUZSzAjCgCbBECo8HMB7H3OAlAAsrJR9vK00dAE9gQG4DQGk5QEYdQHozQD10wA49QHvlQEYndzNLd1cBLGRgBXMIABFTJVMUR2QygE1UcwB9HyMANQBHABVUfU1iHAAvEF6ARU0AYRYp9U4cUWQHOm59HSXOQmQ1KVgANxk3GxZUSm45WAAPJVP3HS6Aa3ajL306pQBXQzGJsMjC9TJpzEolB0dCBkNswPo1PtgHoAppTDp9FYwLUgsBiGQUOJtIp9CE3Mg2ABlACyAAlyNxKAAxASs1lAA==='['\\x73\\x70\\x6c\\x69\\x63']('\\x7c'),0,{})) \u0026lt;/script\u0026gt;   看起来这也是一个函数啊，而且这个Base64的串似乎作为参数了，再次动用chrome的console帮助我们解析一下：\n这下我们大概可以明白几个事情：\n 这段代码实际上是 window[\u0026quot;eval\u0026quot;](...) 省略号部分是一个函数 function(p,a,c,k,e,d), 以 {} 包起来的是函数内容，那return p;} 后面的想必就是这6个参数。  观察一下这6个参数，我们会发现：\n p = 'X.B({\u0026quot;y\u0026quot;:8,\u0026quot;x\u0026quot;:\u0026quot;w v u t s r\u0026quot;,\u0026quot;q\u0026quot;:\u0026quot;8.1\u0026quot;,\u0026quot;p\u0026quot;:o,\u0026quot;n\u0026quot;:\u0026quot;4\u0026quot;,\u0026quot;l\u0026quot;:[\u0026quot;j.1.2\u0026quot;,\u0026quot;A.1.2\u0026quot;,\u0026quot;h.1.2\u0026quot;,\u0026quot;9-a.1.2\u0026quot;,\u0026quot;b.1.2\u0026quot;,\u0026quot;c.1.2\u0026quot;,\u0026quot;d.1.2\u0026quot;,\u0026quot;f.1.2\u0026quot;,\u0026quot;10.1.2\u0026quot;,\u0026quot;11.1.2\u0026quot;,\u0026quot;12.1.2\u0026quot;,\u0026quot;13.1.2\u0026quot;,\u0026quot;14.1.2\u0026quot;,\u0026quot;15.1.2\u0026quot;,\u0026quot;16.1.2\u0026quot;,\u0026quot;17.1.2\u0026quot;,\u0026quot;g.1.2\u0026quot;,\u0026quot;k.1.2\u0026quot;,\u0026quot;C.1.2\u0026quot;,\u0026quot;P.1.2\u0026quot;,\u0026quot;D.1.2\u0026quot;,\u0026quot;U.1.2\u0026quot;,\u0026quot;W.1.2\u0026quot;,\u0026quot;3.1.2\u0026quot;,\u0026quot;%Y%5%7%6%5%Z%6%7%T.1.2\u0026quot;],\u0026quot;V\u0026quot;:R,\u0026quot;Q\u0026quot;:3,\u0026quot;S\u0026quot;:\u0026quot;/O/z/N/4/\u0026quot;,\u0026quot;M\u0026quot;:0,\u0026quot;L\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;K\u0026quot;:J,\u0026quot;I\u0026quot;:H,\u0026quot;G\u0026quot;:{\u0026quot;e\u0026quot;:F,\u0026quot;m\u0026quot;:\u0026quot;E\u0026quot;}}).i();'\n  a = 62, c = 70\n  k = 'D4KwDg5sDuCmBGZgCYCsxA03gBgIyD21YADgCFgBRdATlOQHYtLLgsAWZ9LANmducOGZMc/LAGZgYAE6wAkgDsAlgBdmOYDiYAzBQBtYAZwEBjOQEMAtrGCjULUZSzAjCgCbBECo8HMB7H3OAlAAsrJR9vK00dAE9gQG4DQGk5QEYdQHozQD10wA49QHvlQEYndzNLd1cBLGRgBXMIABFTJVMUR2QygE1UcwB9HyMANQBHABVUfU1iHAAvEF6ARU0AYRYp9U4cUWQHOm59HSXOQmQ1KVgANxk3GxZUSm45WAAPJVP3HS6Aa3ajL306pQBXQzGJsMjC9TJpzEolB0dCBkNswPo1PtgHoAppTDp9FYwLUgsBiGQUOJtIp9CE3Mg2ABlACyAAlyNxKAAxASs1lAA==='['split']('|')\n  e = 0, d = {}\n 唯一需要处理的似乎就是k了，虽然k里没有|这个符号，不过刚才使用LZString.decompressfromBase64()函数解析出来的东西倒是有很多|。\n自此真相大白了，我们需要做的事情很简单：\n 提取出p,a,c,k,e,d这6个参数。 将k放进LZString.decompressfromBase64()解析一下。 调用 decode_func （也就是 function(p,a,c,k,e,d) ），得到结果。  结果长这样：\n我们要的图片链接就找到啦！在 files 里。\nStep 5: 我们还剩下最后一个问题：有了图片链接但是访问不了（403）怎么办？这似乎是一种简单的反爬虫方式，google一下，只要假装我们是从本站（即这个漫画的网站）进去的，而不是从其他地方进去的，就可以访问了。虽然在浏览器上做不到，但是python里可以通过更改Referer的方式来达到：\ndef get_download_header(): return {'User-Agent': random.choice(USER_AGENTS), \u0026quot;Referer\u0026quot;: \u0026quot;https://www.manhuagui.com/comic/27099/\u0026quot;}  Step 6: 最后的最后，就是爬虫的基本过程了，不过我们有一段javascript代码需要运行，怎么在python中运行javascript呢？\n 首先保存一下javascript代码，叫 decode_func.js。内容如下：  const jsdom = require(\u0026quot;jsdom\u0026quot;); const { JSDOM } = jsdom; const dom = new JSDOM(`\u0026lt;!DOCTYPE html\u0026gt;\u0026lt;p\u0026gt;Hello world\u0026lt;/p\u0026gt;`); window = dom.window; document = window.document; XMLHttpRequest = window.XMLHttpRequest; decode_func = window[\u0026quot;eval\u0026quot;](function(p,a,c,k,e,d){e=function(c){return(c\u0026lt;a?\u0026quot;\u0026quot;:e(parseInt(c/a)))+((c=c%a)\u0026gt;35?String.fromCharCode(c+29):c.toString(36))};if(!''.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return'\\\\w+'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp('\\\\b'+e(c)+'\\\\b','g'),k[c]);return p;})  然后用以下这段代码就可以了：  def load_js(): with open(\u0026quot;decode_func.js\u0026quot;, 'r') as file: js = file.read() context = execjs.compile(js, cwd=\u0026quot;/usr/local/lib/node_modules\u0026quot;) return context context = load_js()  调用的时候就用 res = context.call((\u0026quot;decode_func\u0026quot;), p,a,c,k,e,d)\n最终代码如下：\n参考代码 import requests from bs4 import BeautifulSoup import os import time import random import lzstring import execjs import re import json def create_dir(path): if not os.path.exists(path): os.makedirs(path) root_folder = '/Users/huzhenwei/Desktop/manga/' create_dir(root_folder) USER_AGENTS = [ \u0026quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\u0026quot;, \u0026quot;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\u0026quot;, \u0026quot;Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\u0026quot;, \u0026quot;Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\u0026quot;, \u0026quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\u0026quot;, \u0026quot;Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\u0026quot;, \u0026quot;Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\u0026quot;, \u0026quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\u0026quot;, \u0026quot;Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\u0026quot;, \u0026quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\u0026quot;, \u0026quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\u0026quot;, \u0026quot;Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\u0026quot;, \u0026quot;Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\u0026quot;, \u0026quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\u0026quot;, \u0026quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\u0026quot;, \u0026quot;Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52\u0026quot;, ] def get_download_header(): return {'User-Agent': random.choice(USER_AGENTS), \u0026quot;Referer\u0026quot;: \u0026quot;https://www.manhuagui.com/comic/27099/\u0026quot;} def load_js(): with open(\u0026quot;decode_func.js\u0026quot;, 'r') as file: js = file.read() context = execjs.compile(js, cwd=\u0026quot;/usr/local/lib/node_modules\u0026quot;) return context context = load_js() def decode(s): x = lzstring.LZString() decoded_str = x.decompressFromBase64(s) return decoded_str.split(\u0026quot;|\u0026quot;) # p = \u0026quot;\u0026quot;\u0026quot;1h.14({\u0026quot;q\u0026quot;:7,\u0026quot;r\u0026quot;:\u0026quot;s t u v w x\u0026quot;,\u0026quot;y\u0026quot;:\u0026quot;7.1\u0026quot;,\u0026quot;A\u0026quot;:B,\u0026quot;C\u0026quot;:\u0026quot;6\u0026quot;,\u0026quot;D\u0026quot;:[\u0026quot;E.1.2\u0026quot;,\u0026quot;F.1.2\u0026quot;,\u0026quot;G.1.2\u0026quot;,\u0026quot;H.1.2\u0026quot;,\u0026quot;o.1.2\u0026quot;,\u0026quot;I.1.2\u0026quot;,\u0026quot;k.1.2\u0026quot;,\u0026quot;h.1.2\u0026quot;,\u0026quot;d.1.2\u0026quot;,\u0026quot;c.1.2\u0026quot;,\u0026quot;a.1.2\u0026quot;,\u0026quot;9.1.2\u0026quot;,\u0026quot;8.1.2\u0026quot;,\u0026quot;l.1.2\u0026quot;,\u0026quot;b.1.2\u0026quot;,\u0026quot;f.1.2\u0026quot;,\u0026quot;g.1.2\u0026quot;,\u0026quot;i.1.2\u0026quot;,\u0026quot;j.1.2\u0026quot;,\u0026quot;p.1.2\u0026quot;,\u0026quot;J.1.2\u0026quot;,\u0026quot;Y.1.2\u0026quot;,\u0026quot;L.1.2\u0026quot;,\u0026quot;13.1.2\u0026quot;,\u0026quot;K.1.2\u0026quot;,\u0026quot;15.1.2\u0026quot;,\u0026quot;16.1.2\u0026quot;,\u0026quot;17.1.2\u0026quot;,\u0026quot;18.1.2\u0026quot;,\u0026quot;1a.1.2\u0026quot;,\u0026quot;1g.1.2\u0026quot;,\u0026quot;1b.1.2\u0026quot;,\u0026quot;1c.1.2\u0026quot;,\u0026quot;1d.1.2\u0026quot;,\u0026quot;%1e%5%3%4%5%1f%4%3%12.1.2\u0026quot;],\u0026quot;19\u0026quot;:10,\u0026quot;Z\u0026quot;:11,\u0026quot;X\u0026quot;:\u0026quot;/W/z/V/6/\u0026quot;,\u0026quot;U\u0026quot;:0,\u0026quot;T\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;S\u0026quot;:R,\u0026quot;Q\u0026quot;:P,\u0026quot;O\u0026quot;:{\u0026quot;e\u0026quot;:N,\u0026quot;m\u0026quot;:\u0026quot;M\u0026quot;}}).n();\u0026quot;\u0026quot;\u0026quot; # a,c = 62,80 # k=['', 'jpg', 'webp', '9B', 'E5', '8B', '第02回', '27099', 'P0056', 'P0055', 'P0054', 'P0058', 'P0053', 'P0052', '', 'P0059', 'P0060', 'P0051', 'P0061', 'P0062', 'P0050', 'P0057', '', 'preInit', 'P0048', 'P0063', 'bid', 'bname', '总之就是非常可爱', 'fly', 'me', 'to', 'the', 'moon', 'bpic', '', 'cid', '354852', 'cname', 'files', 'P0044', 'P0045', 'P0046', 'P0047', 'P0049', 'P0064', 'P0068', 'P0066', 'GYeIdl7ujUrxJ1ls7JvwpQ', '1612951385', 'sl', '354596', 'prevId', '356912', 'nextId', 'block_cc', 'status', 'zzjsfckafmttm_lj2l', 'ps1', 'path', 'P0065', 'len', 'false', '35', 'BE', 'P0067', 'imgData', 'P0069', 'P0070', 'P0071', 'P0072', 'finished', 'P0073', 'P0075', 'P0076', 'P0077', 'E6', '9F', 'P0074', 'SMH'] # e = 0 # d = dict() # res = context.call((\u0026quot;decode_func\u0026quot;), p,a,c,k,e,d) # print(type(res)) # print(res) # get content of one chapter def get_content(title, url): create_dir(os.path.join(root_folder, title)) res = requests.get(f\u0026quot;https://manhuagui.com{url}\u0026quot;, random.choice(USER_AGENTS)) # print(res.text) soup = BeautifulSoup(res.content, 'html.parser') items = soup.find_all(lambda tag:tag.name=='script', recursive=True) for item in items: txt = item.string # 必须是item.string, 不能是item.txt if txt and \u0026quot;return p;\u0026quot; in txt: # 如果tag里没有文字，txt==None parts = txt.split(\u0026quot;return p;}(\u0026quot;) part = parts[1][:-2] split_res = re.split(r',([0-9]+,[0-9]+,)', part) p = split_res[0][1:-1] split_res[1] = split_res[1][:-1] a, c = map(int, split_res[1].split(',')) k = split_res[2].split(\u0026quot;'['\u0026quot;)[0][1:] k = decode(k) e = 0 d = dict() res = context.call((\u0026quot;decode_func\u0026quot;), p,a,c,k,e,d) s = re.search('({.+})', res).group(0) # 找到一个由 {} 包裹的group info_dict = json.loads(s) files_list = info_dict[\u0026quot;files\u0026quot;] path_prefix = 'https://i.hamreus.com' + info_dict[\u0026quot;path\u0026quot;] i = 1 for file_name in files_list: complete_path = path_prefix + file_name[:-5] print(complete_path) res = requests.get(complete_path, headers=get_download_header()) img_name = os.path.join(root_folder, title, f'{i}.jpg') with open(img_name, 'wb') as file: file.write(res.content) file.flush() time.sleep(random.uniform(5.0, 10.0)) i += 1 # get manga url list def get_url_list(url): res = requests.get(url, random.choice(USER_AGENTS)) soup = BeautifulSoup(res.content, 'html.parser') items = soup.find_all('div', {\u0026quot;id\u0026quot;: \u0026quot;chapter-list-1\u0026quot;}) for manga_list in items: links = manga_list.find_all(\u0026quot;a\u0026quot;, recursive=True) links = links[1:] for link in links: title = link.get(\u0026quot;title\u0026quot;) ref = link.get(\u0026quot;href\u0026quot;) if title[-1] == '卷': continue print(f\u0026quot;{title}, {ref}\u0026quot;) get_content(title, ref) url = \u0026quot;https://www.manhuagui.com/comic/27099/\u0026quot; get_url_list(url)    注：上面这块代码被识别成lua语言了，样式出了点问题，可以在markdown里面指定语言，在第一个 ``` 后面加上语言名即可，如 ```python\n 后记 写这篇blog比玩爬虫本身还累啊，看来我果然不适合写作文（虽然我从小就深刻的明白这个道理）。不过这篇博客很大程度上也是写给自己看的，作为下次爬虫的参考（不知道下次爬虫要等到什么时候了）。\n以后可能会补点儿算法笔记，或者题解之类的。\n","date":"2021-02-01T22:45:03+08:00","permalink":"https://tom0727.github.io/post/002-%E7%88%AC%E8%99%AB/","tags":["爬虫","python"],"title":"初学爬虫小记"},{"categories":["工程"],"contents":"起因 在几天前折腾了爬虫，成功爬下来とにかくかわいい的漫画以后，我发现我折腾似乎上瘾了。聊天时无意提到”要是我有个人网站就好了“，于是就决定动手开始构建咕咕已久的个人网站。\n由于我对自己的前端水平十分有数，所以想都没想就立刻放弃了“要不自己写一个网站？”的想法。\n后来想起之前逛过的ouuan大佬的博客非常好看，我的收藏夹里甚至还有他搭建博客的指南，就直接拿来用了，采用的是 hugo + even主题 + github actions，参考资料如下:\n https://ouuan.gitee.io/post/from-hexo-to-hugo/\n 搭建过程 Step 1 首先阅读ouuan的指南(上述链接)，然后使用他的hugo模版，按照模版里指示的进行clone。\nStep 2 还是按照模版里指示的，修改一下配置文件config.toml里的相关配置，一些需要更改的内容：\n 包含yourname的部分 newContentEditor = \u0026quot;\u0026quot; defaultContentLanguage = \u0026quot;en\u0026quot; [[menu.main]]的相关内容 (视情况进行保留和删除) 不要更改 [params] 中的 version=\u0026quot;4.x\u0026quot;  Step 2.5 因为我打算部署到github pages上，就在github上创建一个新的repository，叫tom0727.github.io\nStep 3 配置完成后，可以 hugo new post/test.md 创建一个新的post(在hugo-blog/content/post/test.md), 按照markdown随便写点东西以后保存，然后 hugo server，打开localhost看一下效果(也可以边写边看效果，热加载真香)。最后用hugo命令生成静态文件，就是hugo-blog/public/文件夹，把这个文件夹内的内容push到github上就可以了。\n注： blog的源代码和网页内容并不是一个东西!\n 源代码: 是hugo-blog/ 下除了hugo-blog/public/以外的内容，包含了 content/, config.toml 之类的文件。 网页内容：只是 hugo-blog/public/内的内容，有了源代码就可以用hugo生成网页内容，但是反之就不可以！  既然两者有别，就要分开管理，我把它们放在同一个repository里，分成2个branch。源代码就放在了master里，网页内容就放在publish上了。\nStep 4 这个时候网页上应该是没有内容的，因为github pages需要设置一下指定deploy的branch，在repository的Settings里，拉到下面看到GitHub Pages，改一下Source branch就可以了：  需要在博文里插入图片的话，假设图片位于 static/images/001/1.png，就写上![image](/images/001/1.png)\n如果是插入link的话，就写 [link_name](https://...) 即可，外部链接记得加https://，不然会被当作本地的某个文件位置。\n 这些步骤做完就可以了，当然这种修改然后发布的方式太麻烦了，切branch也很累，所以就有了Step 5:\nStep 5: 我们配置一下Github actions，它能自动化部署流程。参考资料:\n https://segmentfault.com/a/1190000021815477\n 需要注意，因为源代码和网页内容在同一个repository里，就不用在github上折腾secret key之类的了，直接修改一下 hugo-blog/.github/workflows/deploy.yml (这个是template里自带的) 即可：\n personal_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public publish_branch: publish 将 depth 改成 fetch-depth (不然build的时候会报错)  这样就完成了，从此以后，写一篇新文章的步骤就变成：\n hugo new post/article.md 修改位于content/post/article.md的博客文章 add, commit, 把源代码push到master  这样就可以了，不必切branch然后push网页内容了。\n push到master以后，可以在repository的Actions页面查看一下deploy的情况：  Step 6: 因为github.io似乎被墙了，所以学ouuan弄了一个gitee镜像，教程的话参照这两个就可以了：\n https://jasonkayzk.github.io/2020/09/18/%E5%9C%A8Gitee%E6%90%AD%E5%BB%BAGithub-Pages/\n  https://github.com/yanglbme/gitee-pages-action\n gitee镜像的访问网址：tom0727.gitee.io\n注：在 hugo-blog/.github/workflows/sync.yml里记得设置一下on，不然触发不了自动部署。\n我这里设置的是：\non: push: branches: - master workflow_dispatch:  TODO LIST  打开评论区功能 搞明白baidu push是什么  结语 新的一轮折腾结束了，总体来说还是比较满意这个博客的，个人很喜欢这种极简风的博客，功能也比较全，某种意义上算是告别了在Microsoft Word里做笔记的生活（？），之后打算先补上爬虫的一些笔记，还有搬运一点Word上的笔记吧。\n","date":"2021-02-01T21:21:41+08:00","permalink":"https://tom0727.github.io/post/001-hugo-tutorial/","tags":["hugo","博客"],"title":"Hugo博客搭建小记"},{"categories":null,"contents":"Hello, this is Tom from China! About me  Year 3 Undergraduate student @ HKU (The University of Hong Kong), 2018-2022 Major in Computer Science Competitive programming as hobby Please see my resume at: https://github.com/tom0727/Resume  Contact  Email: huzhenweitom@gmail.com QQ/Wechat: 980409152  Links  Codeforces:  Leetcode: tom0727 ","date":"2021-01-31T22:57:58+08:00","permalink":"https://tom0727.github.io/about/","tags":null,"title":"About"},{"categories":null,"contents":"","date":"2020-02-07T17:43:21+08:00","permalink":"https://tom0727.github.io/search/","tags":null,"title":"搜索"}]