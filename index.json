[{"categories":["技术"],"contents":"写博客好累啊，是我太久没有写作了吗\n这次来记录一下我第一次学习爬虫的经历\n起因 补完とにかくかわいい的番，感觉真好看啊，漫画也不错，就打算补补とにかくかわいい的生肉漫画，但是这网站广告特别多，还会检测我adblocker，禁用javascript的话漫画就加载不出来了，气死我了，一怒之下决定学习爬虫把漫画爬下来看。\n过程 爬虫教程有很多，这里特别推荐一个Jack Cui的教程：\n [资源分享] Python3 网络爬虫：漫画下载，动态加载、反爬虫这都不叫事\n 爬 manga1001.com 这个网站设置的比较粗糙，图片都是静态加载的(F12就能看见图片链接)，根据标签soup.find_all()一下即可。对于这个网站的话，简单说一下爬虫的基本流程吧。\n基本流程\n  观察一下页面的HTML，用F12打开可以看到大致结构，如果要看源代码的话，可以选择：\n1.1. res = requests.get(url), print(res.txt) 或\n1.2. 在url前加上view-source:，然后用浏览器打开。\n  找到包含图片的tag， 找一下规律，然后用 soup.find_all() 即可。\n  获取所有章节的URL，然后分别去每个URL里抓取。\n   参考代码 import requests from bs4 import BeautifulSoup import os import random import time def create_dir(path): if not os.path.exists(path): os.makedirs(path) root_folder = '/Users/huzhenwei/Desktop/manga/' create_dir(root_folder) USER_AGENTS = [ \u0026quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\u0026quot;, \u0026quot;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\u0026quot;, \u0026quot;Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\u0026quot;, \u0026quot;Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\u0026quot;, \u0026quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\u0026quot;, \u0026quot;Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\u0026quot;, \u0026quot;Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\u0026quot;, \u0026quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\u0026quot;, \u0026quot;Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\u0026quot;, \u0026quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\u0026quot;, \u0026quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\u0026quot;, \u0026quot;Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\u0026quot;, \u0026quot;Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\u0026quot;, \u0026quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\u0026quot;, \u0026quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\u0026quot;, \u0026quot;Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52\u0026quot;, ] # get content of one chapter def get_content(folder, prefix, url): res = requests.get(url) soup = BeautifulSoup(res.content, 'html.parser') items = soup.find_all('figure') i = 1 folder_name = os.path.join(folder, f'Chapter_{prefix:03}/') create_dir(folder_name) for item in items: for child in item.children: if i != 1: img_url = child.get('data-src') else: img_url = child.get('src') print(img_url) headers = random.choice(USER_AGENTS) img_html = requests.get(img_url, headers) img_name = os.path.join(folder_name, f'{i:02}.jpg') with open(img_name, 'wb') as file: file.write(img_html.content) file.flush() i += 1 time.sleep(random.uniform(0, 3.33)) # sleep random time # get manga url list def get_url_list(manga_name, url): res = requests.get(url) soup = BeautifulSoup(res.content, 'html.parser') items = soup.find_all('option') for i in range(len(items) - 2, -1, -1): if items[i] == items[-1]: # get manga chapter url list without duplicates items = items[i+1:] break chapter = 1 folder = os.path.join(root_folder, f'{manga_name}/') for item in items: manga_url = item.get('value') get_content(folder, chapter, manga_url) print(manga_url) chapter += 1 def main(): url1 = \u0026quot;https://manga1001.com/%e3%80%90%e7%ac%ac1%e8%a9%b1%e3%80%91%e3%83%88%e3%83%8b%e3%82%ab%e3%82%af%e3%82%ab%e3%83%af%e3%82%a4%e3%82%a4-raw/\u0026quot; name1 = 'Tonikaku_Kawaii' get_url_list(name1, url1) url2 = \u0026quot;https://manga1001.com/%e3%80%90%e7%ac%ac1%e8%a9%b1%e3%80%91%e5%b9%b2%e7%89%a9%e5%a6%b9%e3%81%86%e3%81%be%e3%82%8b%e3%81%a1%e3%82%83%e3%82%93-raw/\u0026quot; name2 = 'Umaru_Chan' get_url_list(name2, url2) main()    注: 这里用的USER_AGENT和sleep()都是为了防止被发现然后封IP\n 爬 manhuagui.com 上面那个太没挑战性了，于是我打算再爬一个。\n打开漫画网站， 发现没有图片链接，说明是动态加载的图片(用javascript加载的)，那怎么办呢？\nStep 1 首先我们要获得某一话的所有图片链接，可以从图上看出似乎有一大段像是加密后的字符串，我们打开第一话和第二话的HTML，用命令行diff一下以后，会发现差异就刚好出现在这串字符串上：我们可以肯定这里面包含了图片链接相关的信息。\nStep 2 太晚了，睡觉去了，明早起来接着写\n","date":"2021-02-01T22:45:03+08:00","permalink":"https://tom0727.github.io/post/002-%E7%88%AC%E8%99%AB/","tags":["爬虫","python"],"title":"初学爬虫小记"},{"categories":["技术"],"contents":"起因 在几天前折腾了爬虫，成功爬下来とにかくかわいい的漫画以后，我发现我折腾似乎上瘾了。聊天时无意提到”要是我有个人网站就好了“，于是就决定动手开始构建咕咕已久的个人网站。\n由于我对自己的前端水平十分有数，所以想都没想就立刻放弃了“要不自己写一个网站？”的想法。\n后来想起之前逛过的ouuan大佬的博客非常好看，我的收藏夹里甚至还有他搭建博客的指南，就直接拿来用了，采用的是 hugo + even主题 + github actions，参考资料如下:\n https://ouuan.gitee.io/post/from-hexo-to-hugo/\n 搭建过程 Step 1 首先阅读ouuan的指南(上述链接)，然后使用他的hugo模版，按照模版里指示的进行clone。\nStep 2 还是按照模版里指示的，修改一下配置文件config.toml里的相关配置，一些需要更改的内容：\n 包含yourname的部分 newContentEditor = \u0026quot;\u0026quot; defaultContentLanguage = \u0026quot;en\u0026quot; [[menu.main]]的相关内容 (视情况进行保留和删除) 不要更改 [params] 中的 version=\u0026quot;4.x\u0026quot;  Step 2.5 因为我打算部署到github pages上，就在github上创建一个新的repository，叫tom0727.github.io\nStep 3 配置完成后，可以 hugo new post/test.md 创建一个新的post(在hugo-blog/content/post/test.md), 按照markdown随便写点东西以后保存，然后 hugo server，打开localhost看一下效果(也可以边写边看效果，热加载真香)。最后用hugo命令生成静态文件，就是hugo-blog/public/文件夹，把这个文件夹内的内容push到github上就可以了。\n注： blog的源代码和网页内容并不是一个东西!\n 源代码: 是hugo-blog/ 下除了hugo-blog/public/以外的内容，包含了 content/, config.toml 之类的文件。 网页内容：只是 hugo-blog/public/内的内容，有了源代码就可以用hugo生成网页内容，但是反之就不可以！  既然两者有别，就要分开管理，我把它们放在同一个repository里，分成2个branch。源代码就放在了master里，网页内容就放在publish上了。\nStep 4 这个时候网页上应该是没有内容的，因为github pages需要设置一下指定deploy的branch，在repository的Settings里，拉到下面看到GitHub Pages，改一下Source branch就可以了：  需要在博文里插入图片的话，假设图片位于 static/images/001/1.png，就写上![image](/images/001/1.png)\n如果是插入link的话，就写 [link_name](https://...) 即可，外部链接记得加https://，不然会被当作本地的某个文件位置。\n 这些步骤做完就可以了，当然这种修改然后发布的方式太麻烦了，切branch也很累，所以就有了Step 5:\nStep 5: 我们配置一下Github actions，它能自动化部署流程。参考资料:\n https://segmentfault.com/a/1190000021815477\n 需要注意，因为源代码和网页内容在同一个repository里，就不用在github上折腾secret key之类的了，直接修改一下 hugo-blog/.github/workflows/deploy.yml (这个是template里自带的) 即可：\n personal_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public publish_branch: publish 将 depth 改成 fetch-depth (不然build的时候会报错)  这样就完成了，从此以后，写一篇新文章的步骤就变成：\n hugo new post/article.md 修改位于content/post/article.md的博客文章 add, commit, 把源代码push到master  这样就可以了，不必切branch然后push网页内容了。\n push到master以后，可以在repository的Actions页面查看一下deploy的情况：  Step 6: 因为github.io似乎被墙了，所以学ouuan弄了一个gitee镜像，教程的话参照这两个就可以了：\n https://jasonkayzk.github.io/2020/09/18/%E5%9C%A8Gitee%E6%90%AD%E5%BB%BAGithub-Pages/\n  https://github.com/yanglbme/gitee-pages-action\n gitee镜像的访问网址：tom0727.gitee.io\n注：在 hugo-blog/.github/workflows/sync.yml里记得设置一下on，不然触发不了自动部署。\n我这里设置的是：\non: push: branches: - master workflow_dispatch:  TODO LIST  打开评论区功能 搞明白baidu push是什么  结语 新的一轮折腾结束了，总体来说还是比较满意这个博客的，个人很喜欢这种极简风的博客，功能也比较全，某种意义上算是告别了在Microsoft Word里做笔记的生活（？），之后打算先补上爬虫的一些笔记，还有搬运一点Word上的笔记吧。\n","date":"2021-02-01T21:21:41+08:00","permalink":"https://tom0727.github.io/post/001-hugo-tutorial/","tags":["hugo","博客"],"title":"Hugo博客搭建小记"},{"categories":null,"contents":"Hello, this is Tom from China! About me  Year 3 Undergraduate student @ HKU (The University of Hong Kong), 2018-2022 Major in Computer Science Competitive programming as hobby  Contact  Email: huzhenweitom@gmail.com QQ/Wechat: 980409152  Links  Codeforces:  Leetcode: tom0727 ","date":"2021-01-31T22:57:58+08:00","permalink":"https://tom0727.github.io/about/","tags":null,"title":"About"},{"categories":null,"contents":"","date":"2020-02-07T17:43:21+08:00","permalink":"https://tom0727.github.io/search/","tags":null,"title":"搜索"}]